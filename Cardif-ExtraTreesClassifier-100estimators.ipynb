{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn import ensemble\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn import metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "common_drop_columns = ['ID','v8','v23','v25','v31','v36','v37','v46',\n",
    "                       'v51','v53','v54','v63','v73','v75','v79','v81',\n",
    "                       'v82','v89','v92','v95','v105','v107','v108','v109',\n",
    "                       'v110','v116','v117','v118','v119','v123','v124','v128']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the training data - store the target separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded TRAIN data...\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "target = train['target'].values\n",
    "\n",
    "#Remove 'drop' columns\n",
    "train_drop_columns  = common_drop_columns[:] + ['target']\n",
    "train = train.drop(train_drop_columns,axis=1)\n",
    "print('Loaded TRAIN data...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded TEST data...\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "id_test = test['ID'].values\n",
    "#Remove 'drop' columns\n",
    "test = test.drop(common_drop_columns,axis=1)\n",
    "print('Loaded TEST data...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\apps\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:8: FutureWarning: pd.rolling_apply is deprecated for Series and will be removed in a future version, replace with \n",
      "\tSeries.rolling(center=False,window=2).apply(kwargs=<dict>,args=<tuple>,func=<function>)\n"
     ]
    }
   ],
   "source": [
    "def find_delimiter(df, col):\n",
    "    #Function that trying to find an approximate delimiter used for scaling.\n",
    "    #So we can undo the feature scaling.\n",
    "\n",
    "    vals = df[col].dropna()\n",
    "\n",
    "    vals = vals.sort_values().round(8)\n",
    "    vals = pd.rolling_apply(vals, 2, lambda x: x[1] - x[0])\n",
    "    vals = vals[vals > 0.000001]\n",
    "    return vals.value_counts().idxmax() \n",
    "\n",
    "num_vars = ['v1', 'v2', 'v4', 'v5', 'v6', 'v7', 'v9', 'v10', 'v11',\n",
    "            'v12', 'v13', 'v14', 'v15', 'v16', 'v17', 'v18', 'v19', 'v20',\n",
    "            'v21', 'v26', 'v27', 'v28', 'v29', 'v32', 'v33', 'v34', 'v35', 'v38',\n",
    "            'v39', 'v40', 'v41', 'v42', 'v43', 'v44', 'v45', 'v48', 'v49', 'v50',\n",
    "            'v55', 'v57', 'v58', 'v59', 'v60', 'v61', 'v62', 'v64', 'v65', 'v67',\n",
    "            'v68', 'v69', 'v70', 'v72', 'v76', 'v77', 'v78', 'v80', 'v83', 'v84', \n",
    "            'v85', 'v86', 'v87', 'v88', 'v90', 'v93', 'v94', 'v96', 'v97', 'v98', \n",
    "            'v99', 'v100', 'v101', 'v102', 'v103', 'v104', 'v106', 'v111', 'v114',\n",
    "            'v115', 'v120', 'v121', 'v122', 'v126', 'v127', 'v129', 'v130', 'v131']\n",
    "\n",
    "vs = pd.concat([train, test])\n",
    "for c in num_vars:\n",
    "    if c not in train.columns:\n",
    "        continue\n",
    "    \n",
    "    train.loc[train[c].round(5) == 0, c] = 0\n",
    "    test.loc[test[c].round(5) == 0, c] = 0\n",
    "\n",
    "    delimiter = find_delimiter(vs, c)\n",
    "    train[c] *= 1/delimiter\n",
    "    test[c] *= 1/delimiter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_feature_importance(et_model, feature_count):\n",
    "\n",
    "    importances = et_model.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "                            \n",
    "    plt.figure()\n",
    "    plt.title(\"Feature importances\")\n",
    "    plt.bar(range(feature_count), \n",
    "            importances[indices],\n",
    "            color=\"r\", \n",
    "            yerr=std[indices], \n",
    "            align=\"center\")\n",
    "    \n",
    "    plt.xticks(range(feature_count), indices)\n",
    "    plt.xlim([-1, feature_count])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_feature_ranking(et_model,df_train):\n",
    "\n",
    "    importances = et_model.feature_importances_\n",
    "    std = np.std([tree.feature_importances_ for tree in et_model.estimators_],\n",
    "                 axis=0)\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "\n",
    "    # Print the feature ranking\n",
    "    print(\"Feature ranking:\")\n",
    "\n",
    "    for f in range(df_train.shape[1]):\n",
    "        print(\"%d. feature %d:%s (%f)\" % (f + 1, indices[f], df_train.columns.values[indices[f]], importances[indices[f]]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_model_stats(df_model,df_train, y_target):\n",
    "\n",
    "    scores = cross_val_score(df_model,df_train,y_target,n_jobs=3)\n",
    "    print (\"Cross Val Score %f\" % (scores.mean()))\n",
    "\n",
    "    y_hat_train = df_model.predict_proba(df_train)\n",
    "    print(df_model)\n",
    "    print( \"\\nlog-loss train %f\" %(metrics.log_loss(y_target,y_hat_train)))\n",
    "\n",
    "    predictions = df_model.predict(df_train)\n",
    "\n",
    "    #print(metrics.classification_report(target, predictions))\n",
    "    print (pd.crosstab(y_target, predictions, rownames=['True'], colnames=['Predicted'], margins=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for (train_name, train_series), (test_name, test_series) in zip(train.iteritems(),test.iteritems()):\n",
    "    if train_series.dtype == 'O':\n",
    "        #for objects: factorize\n",
    "        train[train_name], tmp_indexer = pd.factorize(train[train_name])\n",
    "        test[test_name] = tmp_indexer.get_indexer(test[test_name])\n",
    "        #but now we have -1 values (NaN)\n",
    "    else:\n",
    "        #for int or float: fill NaN\n",
    "        tmp_len = len(train[train_series.isnull()])\n",
    "        if tmp_len>0:\n",
    "            #print \"mean\", train_series.mean()\n",
    "            train.loc[train_series.isnull(), train_name] = -999 \n",
    "        #and Test\n",
    "        tmp_len = len(test[test_series.isnull()])\n",
    "        if tmp_len>0:\n",
    "            test.loc[test_series.isnull(), test_name] = -999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = train\n",
    "X_test = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Training...')\n",
    "\n",
    "#extc = ExtraTreesClassifier(n_estimators=750,max_features= 70,criterion= 'entropy',min_samples_split= 4,\n",
    "#                            max_depth= 45, min_samples_leaf= 1, n_jobs = 2)     \n",
    "\n",
    "\n",
    "extc = ExtraTreesClassifier(n_estimators=10,max_features= 70,criterion= 'entropy',min_samples_split= 4,\n",
    "                            max_depth= 45, min_samples_leaf= 1, n_jobs = 2)     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy',\n",
       "           max_depth=45, max_features=70, max_leaf_nodes=None,\n",
       "           min_samples_leaf=1, min_samples_split=4,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=2,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extc.fit(X_train,target) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature ranking:\n",
      "1. feature 42:v50 (0.094886)\n",
      "2. feature 43:v52 (0.028987)\n",
      "3. feature 12:v14 (0.028704)\n",
      "4. feature 54:v66 (0.027600)\n",
      "5. feature 20:v22 (0.027172)\n",
      "6. feature 87:v112 (0.027154)\n",
      "7. feature 94:v125 (0.027039)\n",
      "8. feature 73:v91 (0.026909)\n",
      "9. feature 39:v47 (0.026547)\n",
      "10. feature 89:v114 (0.026178)\n",
      "11. feature 8:v10 (0.025884)\n",
      "12. feature 10:v12 (0.025615)\n",
      "13. feature 45:v56 (0.025522)\n",
      "14. feature 19:v21 (0.025445)\n",
      "15. feature 33:v40 (0.025202)\n",
      "16. feature 29:v34 (0.023708)\n",
      "17. feature 88:v113 (0.021476)\n",
      "18. feature 21:v24 (0.020885)\n",
      "19. feature 26:v30 (0.019314)\n",
      "20. feature 59:v71 (0.018944)\n",
      "21. feature 51:v62 (0.017454)\n",
      "22. feature 60:v72 (0.012019)\n",
      "23. feature 97:v129 (0.010439)\n",
      "24. feature 70:v87 (0.007485)\n",
      "25. feature 78:v98 (0.007289)\n",
      "26. feature 91:v120 (0.007097)\n",
      "27. feature 4:v5 (0.006991)\n",
      "28. feature 0:v1 (0.006919)\n",
      "29. feature 58:v70 (0.006835)\n",
      "30. feature 24:v28 (0.006742)\n",
      "31. feature 99:v131 (0.006690)\n",
      "32. feature 32:v39 (0.006440)\n",
      "33. feature 79:v99 (0.006318)\n",
      "34. feature 96:v127 (0.006192)\n",
      "35. feature 71:v88 (0.006184)\n",
      "36. feature 1:v2 (0.006055)\n",
      "37. feature 47:v58 (0.005983)\n",
      "38. feature 5:v6 (0.005948)\n",
      "39. feature 65:v80 (0.005917)\n",
      "40. feature 68:v85 (0.005903)\n",
      "41. feature 14:v16 (0.005728)\n",
      "42. feature 57:v69 (0.005710)\n",
      "43. feature 16:v18 (0.005673)\n",
      "44. feature 93:v122 (0.005671)\n",
      "45. feature 82:v102 (0.005668)\n",
      "46. feature 80:v100 (0.005630)\n",
      "47. feature 90:v115 (0.005620)\n",
      "48. feature 64:v78 (0.005568)\n",
      "49. feature 95:v126 (0.005566)\n",
      "50. feature 23:v27 (0.005450)\n",
      "51. feature 38:v45 (0.005339)\n",
      "52. feature 72:v90 (0.005325)\n",
      "53. feature 7:v9 (0.005302)\n",
      "54. feature 36:v43 (0.005288)\n",
      "55. feature 67:v84 (0.005265)\n",
      "56. feature 30:v35 (0.005205)\n",
      "57. feature 81:v101 (0.005177)\n",
      "58. feature 22:v26 (0.005104)\n",
      "59. feature 46:v57 (0.005087)\n",
      "60. feature 6:v7 (0.005067)\n",
      "61. feature 86:v111 (0.005013)\n",
      "62. feature 77:v97 (0.005009)\n",
      "63. feature 49:v60 (0.004986)\n",
      "64. feature 69:v86 (0.004982)\n",
      "65. feature 44:v55 (0.004978)\n",
      "66. feature 27:v32 (0.004940)\n",
      "67. feature 83:v103 (0.004910)\n",
      "68. feature 13:v15 (0.004848)\n",
      "69. feature 98:v130 (0.004773)\n",
      "70. feature 37:v44 (0.004746)\n",
      "71. feature 48:v59 (0.004664)\n",
      "72. feature 92:v121 (0.004656)\n",
      "73. feature 75:v94 (0.004635)\n",
      "74. feature 35:v42 (0.004609)\n",
      "75. feature 84:v104 (0.004597)\n",
      "76. feature 66:v83 (0.004552)\n",
      "77. feature 9:v11 (0.004518)\n",
      "78. feature 3:v4 (0.004501)\n",
      "79. feature 11:v13 (0.004415)\n",
      "80. feature 74:v93 (0.004366)\n",
      "81. feature 56:v68 (0.004334)\n",
      "82. feature 17:v19 (0.004281)\n",
      "83. feature 62:v76 (0.004245)\n",
      "84. feature 63:v77 (0.004219)\n",
      "85. feature 52:v64 (0.004187)\n",
      "86. feature 28:v33 (0.004160)\n",
      "87. feature 15:v17 (0.004148)\n",
      "88. feature 18:v20 (0.004100)\n",
      "89. feature 85:v106 (0.003934)\n",
      "90. feature 41:v49 (0.003920)\n",
      "91. feature 50:v61 (0.003910)\n",
      "92. feature 40:v48 (0.003820)\n",
      "93. feature 76:v96 (0.003687)\n",
      "94. feature 25:v29 (0.003647)\n",
      "95. feature 53:v65 (0.003619)\n",
      "96. feature 55:v67 (0.003586)\n",
      "97. feature 34:v41 (0.003402)\n",
      "98. feature 31:v38 (0.002948)\n",
      "99. feature 2:v3 (0.001656)\n",
      "100. feature 61:v74 (0.000986)\n"
     ]
    }
   ],
   "source": [
    "print_feature_ranking(extc, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the feature importances of the forest\n",
    "\n",
    "plot_feature_importance(extc, X_train.shape[1])                          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Calculate the Cross Validation Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Val Score 0.763071\n",
      "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy',\n",
      "           max_depth=45, max_features=70, max_leaf_nodes=None,\n",
      "           min_samples_leaf=1, min_samples_split=4,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=2,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "\n",
      "log-loss train 0.026915\n",
      "Predicted      0      1     All\n",
      "True                           \n",
      "0          27294      6   27300\n",
      "1              2  87019   87021\n",
      "All        27296  87025  114321\n"
     ]
    }
   ],
   "source": [
    "print_model_stats(extc, X_train,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Val Score 0.786111\n",
      "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy',\n",
      "           max_depth=45, max_features=40, max_leaf_nodes=None,\n",
      "           min_samples_leaf=1, min_samples_split=4,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=2,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "\n",
      "log-loss train 0.036256\n",
      "Predicted      0      1     All\n",
      "True                           \n",
      "0          27299      1   27300\n",
      "1              6  87015   87021\n",
      "All        27305  87016  114321\n",
      "Cross Val Score 0.785333\n",
      "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy',\n",
      "           max_depth=45, max_features=50, max_leaf_nodes=None,\n",
      "           min_samples_leaf=1, min_samples_split=4,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=2,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "\n",
      "log-loss train 0.032211\n",
      "Predicted      0      1     All\n",
      "True                           \n",
      "0          27299      1   27300\n",
      "1              4  87017   87021\n",
      "All        27303  87018  114321\n",
      "Cross Val Score 0.784974\n",
      "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy',\n",
      "           max_depth=45, max_features=60, max_leaf_nodes=None,\n",
      "           min_samples_leaf=1, min_samples_split=4,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=2,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "\n",
      "log-loss train 0.029147\n",
      "Predicted      0      1     All\n",
      "True                           \n",
      "0          27299      1   27300\n",
      "1              5  87016   87021\n",
      "All        27304  87017  114321\n",
      "Cross Val Score 0.785271\n",
      "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy',\n",
      "           max_depth=45, max_features=70, max_leaf_nodes=None,\n",
      "           min_samples_leaf=1, min_samples_split=4,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=2,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "\n",
      "log-loss train 0.027197\n",
      "Predicted      0      1     All\n",
      "True                           \n",
      "0          27297      3   27300\n",
      "1              3  87018   87021\n",
      "All        27300  87021  114321\n",
      "Cross Val Score 0.783574\n",
      "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy',\n",
      "           max_depth=45, max_features=80, max_leaf_nodes=None,\n",
      "           min_samples_leaf=1, min_samples_split=4,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=2,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "\n",
      "log-loss train 0.025458\n",
      "Predicted      0      1     All\n",
      "True                           \n",
      "0          27299      1   27300\n",
      "1              5  87016   87021\n",
      "All        27304  87017  114321\n",
      "Cross Val Score 0.784703\n",
      "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy',\n",
      "           max_depth=45, max_features=90, max_leaf_nodes=None,\n",
      "           min_samples_leaf=1, min_samples_split=4,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=2,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "\n",
      "log-loss train 0.024409\n",
      "Predicted      0      1     All\n",
      "True                           \n",
      "0          27299      1   27300\n",
      "1              5  87016   87021\n",
      "All        27304  87017  114321\n",
      "Cross Val Score 0.785324\n",
      "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "           max_depth=45, max_features=40, max_leaf_nodes=None,\n",
      "           min_samples_leaf=1, min_samples_split=4,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=2,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "\n",
      "log-loss train 0.044084\n",
      "Predicted      0      1     All\n",
      "True                           \n",
      "0          27297      3   27300\n",
      "1              4  87017   87021\n",
      "All        27301  87020  114321\n",
      "Cross Val Score 0.785245\n",
      "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "           max_depth=45, max_features=50, max_leaf_nodes=None,\n",
      "           min_samples_leaf=1, min_samples_split=4,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=2,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "\n",
      "log-loss train 0.040425\n",
      "Predicted      0      1     All\n",
      "True                           \n",
      "0          27299      1   27300\n",
      "1              5  87016   87021\n",
      "All        27304  87017  114321\n",
      "Cross Val Score 0.784886\n",
      "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "           max_depth=45, max_features=60, max_leaf_nodes=None,\n",
      "           min_samples_leaf=1, min_samples_split=4,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=2,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "\n",
      "log-loss train 0.037806\n",
      "Predicted      0      1     All\n",
      "True                           \n",
      "0          27297      3   27300\n",
      "1              3  87018   87021\n",
      "All        27300  87021  114321\n",
      "Cross Val Score 0.784738\n",
      "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "           max_depth=45, max_features=70, max_leaf_nodes=None,\n",
      "           min_samples_leaf=1, min_samples_split=4,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=2,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "\n",
      "log-loss train 0.035857\n",
      "Predicted      0      1     All\n",
      "True                           \n",
      "0          27299      1   27300\n",
      "1              5  87016   87021\n",
      "All        27304  87017  114321\n",
      "Cross Val Score 0.784545\n",
      "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "           max_depth=45, max_features=80, max_leaf_nodes=None,\n",
      "           min_samples_leaf=1, min_samples_split=4,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=2,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "\n",
      "log-loss train 0.034211\n",
      "Predicted      0      1     All\n",
      "True                           \n",
      "0          27299      1   27300\n",
      "1              5  87016   87021\n",
      "All        27304  87017  114321\n",
      "Cross Val Score 0.783321\n",
      "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "           max_depth=45, max_features=90, max_leaf_nodes=None,\n",
      "           min_samples_leaf=1, min_samples_split=4,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=2,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "\n",
      "log-loss train 0.032800\n",
      "Predicted      0      1     All\n",
      "True                           \n",
      "0          27298      2   27300\n",
      "1              4  87017   87021\n",
      "All        27302  87019  114321\n",
      "Cross Val Score 0.786872\n",
      "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy',\n",
      "           max_depth=45, max_features=40, max_leaf_nodes=None,\n",
      "           min_samples_leaf=1, min_samples_split=4,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=2,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "\n",
      "log-loss train 0.035744\n",
      "Predicted      0      1     All\n",
      "True                           \n",
      "0          27298      2   27300\n",
      "1              3  87018   87021\n",
      "All        27301  87020  114321\n",
      "Cross Val Score 0.786741\n",
      "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy',\n",
      "           max_depth=45, max_features=50, max_leaf_nodes=None,\n",
      "           min_samples_leaf=1, min_samples_split=4,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=2,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "\n",
      "log-loss train 0.031789\n",
      "Predicted      0      1     All\n",
      "True                           \n",
      "0          27298      2   27300\n",
      "1              4  87017   87021\n",
      "All        27302  87019  114321\n",
      "Cross Val Score 0.786925\n",
      "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy',\n",
      "           max_depth=45, max_features=60, max_leaf_nodes=None,\n",
      "           min_samples_leaf=1, min_samples_split=4,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=2,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "\n",
      "log-loss train 0.028956\n",
      "Predicted      0      1     All\n",
      "True                           \n",
      "0          27298      2   27300\n",
      "1              4  87017   87021\n",
      "All        27302  87019  114321\n",
      "Cross Val Score 0.785980\n",
      "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy',\n",
      "           max_depth=45, max_features=70, max_leaf_nodes=None,\n",
      "           min_samples_leaf=1, min_samples_split=4,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=2,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "\n",
      "log-loss train 0.027194\n",
      "Predicted      0      1     All\n",
      "True                           \n",
      "0          27298      2   27300\n",
      "1              4  87017   87021\n",
      "All        27302  87019  114321\n",
      "Cross Val Score 0.785717\n",
      "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy',\n",
      "           max_depth=45, max_features=80, max_leaf_nodes=None,\n",
      "           min_samples_leaf=1, min_samples_split=4,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=2,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "\n",
      "log-loss train 0.025591\n",
      "Predicted      0      1     All\n",
      "True                           \n",
      "0          27298      2   27300\n",
      "1              4  87017   87021\n",
      "All        27302  87019  114321\n",
      "Cross Val Score 0.785700\n",
      "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy',\n",
      "           max_depth=45, max_features=90, max_leaf_nodes=None,\n",
      "           min_samples_leaf=1, min_samples_split=4,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=2,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "\n",
      "log-loss train 0.024440\n",
      "Predicted      0      1     All\n",
      "True                           \n",
      "0          27299      1   27300\n",
      "1              5  87016   87021\n",
      "All        27304  87017  114321\n",
      "Cross Val Score 0.786688\n",
      "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "           max_depth=45, max_features=40, max_leaf_nodes=None,\n",
      "           min_samples_leaf=1, min_samples_split=4,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=2,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "\n",
      "log-loss train 0.044016\n",
      "Predicted      0      1     All\n",
      "True                           \n",
      "0          27298      2   27300\n",
      "1              4  87017   87021\n",
      "All        27302  87019  114321\n",
      "Cross Val Score 0.787231\n",
      "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "           max_depth=45, max_features=50, max_leaf_nodes=None,\n",
      "           min_samples_leaf=1, min_samples_split=4,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=2,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "\n",
      "log-loss train 0.040304\n",
      "Predicted      0      1     All\n",
      "True                           \n",
      "0          27299      1   27300\n",
      "1              5  87016   87021\n",
      "All        27304  87017  114321\n",
      "Cross Val Score 0.786653\n",
      "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "           max_depth=45, max_features=60, max_leaf_nodes=None,\n",
      "           min_samples_leaf=1, min_samples_split=4,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=2,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "\n",
      "log-loss train 0.037645\n",
      "Predicted      0      1     All\n",
      "True                           \n",
      "0          27298      2   27300\n",
      "1              4  87017   87021\n",
      "All        27302  87019  114321\n",
      "Cross Val Score 0.785936\n",
      "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "           max_depth=45, max_features=70, max_leaf_nodes=None,\n",
      "           min_samples_leaf=1, min_samples_split=4,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=2,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "\n",
      "log-loss train 0.035662\n",
      "Predicted      0      1     All\n",
      "True                           \n",
      "0          27298      2   27300\n",
      "1              4  87017   87021\n",
      "All        27302  87019  114321\n",
      "Cross Val Score 0.785254\n",
      "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "           max_depth=45, max_features=80, max_leaf_nodes=None,\n",
      "           min_samples_leaf=1, min_samples_split=4,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=2,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "\n",
      "log-loss train 0.034093\n",
      "Predicted      0      1     All\n",
      "True                           \n",
      "0          27300      0   27300\n",
      "1              6  87015   87021\n",
      "All        27306  87015  114321\n",
      "Cross Val Score 0.784703\n",
      "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "           max_depth=45, max_features=90, max_leaf_nodes=None,\n",
      "           min_samples_leaf=1, min_samples_split=4,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=2,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "\n",
      "log-loss train 0.032902\n",
      "Predicted      0      1     All\n",
      "True                           \n",
      "0          27298      2   27300\n",
      "1              4  87017   87021\n",
      "All        27302  87019  114321\n"
     ]
    }
   ],
   "source": [
    "# Loop through a  couple of training option\n",
    "\n",
    "for i in [80,90]:\n",
    "    extc = ExtraTreesClassifier(n_estimators=100,max_features= i,criterion= 'entropy',min_samples_split= 4,\n",
    "                                max_depth= 45, min_samples_leaf= 1, n_jobs = 2)     \n",
    "    extc.fit(X_train,target) \n",
    "    print_model_stats(extc, X_train,target)\n",
    "    \n",
    "for i in [40,50,60,70,80,90]:\n",
    "    extc = ExtraTreesClassifier(n_estimators=100,max_features= i,criterion= 'gini',min_samples_split= 4,\n",
    "                                max_depth= 45, min_samples_leaf= 1, n_jobs = 2)     \n",
    "    extc.fit(X_train,target) \n",
    "    print_model_stats(extc, X_train,target)\n",
    "    \n",
    "for i in [40,50,60,70,80,90]:\n",
    "    extc = ExtraTreesClassifier(n_estimators=500,max_features= i,criterion= 'entropy',min_samples_split= 4,\n",
    "                                max_depth= 45, min_samples_leaf= 1, n_jobs = 2)     \n",
    "    extc.fit(X_train,target) \n",
    "    print_model_stats(extc, X_train,target)\n",
    "    \n",
    "for i in [40,50,60,70,80,90]:\n",
    "    extc = ExtraTreesClassifier(n_estimators=500,max_features= i,criterion= 'gini',min_samples_split= 4,\n",
    "                                max_depth= 45, min_samples_leaf= 1, n_jobs = 2)     \n",
    "    extc.fit(X_train,target) \n",
    "    print_model_stats(extc, X_train,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'extra_trees_entropy_10_10_10.csv'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "depth = 10\n",
    "features = 10 \n",
    "estimators = 10\n",
    "\n",
    "my_file = \"extra_trees_entropy_\" + str(depth) + \"_\" + str(features) + \"_\" + str(estimators) + \".csv\"\n",
    "my_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Val Score 0.785210\n",
      "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy',\n",
      "           max_depth=45, max_features=90, max_leaf_nodes=None,\n",
      "           min_samples_leaf=1, min_samples_split=4,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=750, n_jobs=2,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "\n",
      "log-loss train 0.024302\n",
      "Predicted      0      1     All\n",
      "True                           \n",
      "0          27299      1   27300\n",
      "1              5  87016   87021\n",
      "All        27304  87017  114321\n",
      "Cross Val Score 0.785298\n",
      "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy',\n",
      "           max_depth=45, max_features=95, max_leaf_nodes=None,\n",
      "           min_samples_leaf=1, min_samples_split=4,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=2,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "\n",
      "log-loss train 0.023851\n",
      "Predicted      0      1     All\n",
      "True                           \n",
      "0          27298      2   27300\n",
      "1              4  87017   87021\n",
      "All        27302  87019  114321\n"
     ]
    }
   ],
   "source": [
    "for depth,features,estimators in [(45,90,750),(45,95,1000)]:\n",
    "    extc = ExtraTreesClassifier(n_estimators=estimators,\n",
    "                                max_features= features,\n",
    "                                criterion= 'entropy',\n",
    "                                min_samples_split= 4,\n",
    "                                max_depth= depth, \n",
    "                                min_samples_leaf= 1, \n",
    "                                n_jobs = 2)     \n",
    "    extc.fit(X_train,target) \n",
    "    print_model_stats(extc, X_train,target)\n",
    "    y_pred = extc.predict_proba(X_test)\n",
    "    my_file = \"extra_trees_entropy_\" + str(depth) + \"_\" + str(features) + \"_\" + str(estimators) + \".csv\"\n",
    "    pd.DataFrame({\"ID\": id_test, \"PredictedProb\": y_pred[:,1]}).to_csv(my_file,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = extc.predict_proba(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame({\"ID\": id_test, \"PredictedProb\": y_pred[:,1]}).to_csv('extra_trees_entropy_750.csv',index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>v3</th>\n",
       "      <th>v4</th>\n",
       "      <th>v5</th>\n",
       "      <th>v6</th>\n",
       "      <th>v7</th>\n",
       "      <th>v9</th>\n",
       "      <th>v10</th>\n",
       "      <th>v11</th>\n",
       "      <th>...</th>\n",
       "      <th>v115</th>\n",
       "      <th>v120</th>\n",
       "      <th>v121</th>\n",
       "      <th>v122</th>\n",
       "      <th>v125</th>\n",
       "      <th>v126</th>\n",
       "      <th>v127</th>\n",
       "      <th>v129</th>\n",
       "      <th>v130</th>\n",
       "      <th>v131</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.143930e+05</td>\n",
       "      <td>1.143930e+05</td>\n",
       "      <td>114393.000000</td>\n",
       "      <td>1.143930e+05</td>\n",
       "      <td>114393.000000</td>\n",
       "      <td>1.143930e+05</td>\n",
       "      <td>1.143930e+05</td>\n",
       "      <td>1.143930e+05</td>\n",
       "      <td>114393.000000</td>\n",
       "      <td>1.143930e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>1.143930e+05</td>\n",
       "      <td>1.143930e+05</td>\n",
       "      <td>1.143930e+05</td>\n",
       "      <td>1.143930e+05</td>\n",
       "      <td>114393.000000</td>\n",
       "      <td>1.143930e+05</td>\n",
       "      <td>1.143930e+05</td>\n",
       "      <td>114393.000000</td>\n",
       "      <td>1.143930e+05</td>\n",
       "      <td>1.143930e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>9.000464e+05</td>\n",
       "      <td>4.154333e+06</td>\n",
       "      <td>-0.027764</td>\n",
       "      <td>2.290731e+06</td>\n",
       "      <td>322.415338</td>\n",
       "      <td>1.333313e+06</td>\n",
       "      <td>1.358637e+06</td>\n",
       "      <td>5.042254e+06</td>\n",
       "      <td>85.256738</td>\n",
       "      <td>8.457066e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>5.768635e+06</td>\n",
       "      <td>7.092238e+05</td>\n",
       "      <td>1.516701e+06</td>\n",
       "      <td>3.809128e+06</td>\n",
       "      <td>33.223598</td>\n",
       "      <td>8.570540e+05</td>\n",
       "      <td>1.789827e+06</td>\n",
       "      <td>0.307589</td>\n",
       "      <td>1.067879e+06</td>\n",
       "      <td>9.585897e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.118774e+06</td>\n",
       "      <td>4.256947e+06</td>\n",
       "      <td>0.182975</td>\n",
       "      <td>2.184290e+06</td>\n",
       "      <td>1159.870074</td>\n",
       "      <td>1.248859e+06</td>\n",
       "      <td>1.271173e+06</td>\n",
       "      <td>4.659935e+06</td>\n",
       "      <td>70.154956</td>\n",
       "      <td>7.459422e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>5.263614e+06</td>\n",
       "      <td>8.248634e+05</td>\n",
       "      <td>1.669347e+06</td>\n",
       "      <td>3.605492e+06</td>\n",
       "      <td>21.666883</td>\n",
       "      <td>8.291153e+05</td>\n",
       "      <td>1.975980e+06</td>\n",
       "      <td>0.686654</td>\n",
       "      <td>1.326276e+06</td>\n",
       "      <td>1.188139e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-9.990000e+02</td>\n",
       "      <td>-9.990000e+02</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-9.990000e+02</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>-9.990000e+02</td>\n",
       "      <td>-9.990000e+02</td>\n",
       "      <td>-9.990000e+02</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>-9.990000e+02</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.990000e+02</td>\n",
       "      <td>-9.990000e+02</td>\n",
       "      <td>-9.990000e+02</td>\n",
       "      <td>-9.990000e+02</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-9.990000e+02</td>\n",
       "      <td>-9.990000e+02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-9.990000e+02</td>\n",
       "      <td>-9.990000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-9.990000e+02</td>\n",
       "      <td>-9.990000e+02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-9.990000e+02</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>-9.990000e+02</td>\n",
       "      <td>-9.990000e+02</td>\n",
       "      <td>-9.990000e+02</td>\n",
       "      <td>48.004382</td>\n",
       "      <td>-9.990000e+02</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.990000e+02</td>\n",
       "      <td>-9.990000e+02</td>\n",
       "      <td>-9.990000e+02</td>\n",
       "      <td>-9.990000e+02</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>-9.990000e+02</td>\n",
       "      <td>-9.990000e+02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-9.990000e+02</td>\n",
       "      <td>-9.990000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.224367e+05</td>\n",
       "      <td>4.009687e+06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.692821e+06</td>\n",
       "      <td>992.289114</td>\n",
       "      <td>1.723894e+06</td>\n",
       "      <td>1.762169e+06</td>\n",
       "      <td>6.734066e+06</td>\n",
       "      <td>60.005470</td>\n",
       "      <td>1.413161e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>8.299406e+06</td>\n",
       "      <td>5.425478e+05</td>\n",
       "      <td>1.357467e+06</td>\n",
       "      <td>4.727809e+06</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>1.146174e+06</td>\n",
       "      <td>1.493776e+06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.310603e+05</td>\n",
       "      <td>5.754002e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.558519e+06</td>\n",
       "      <td>7.446121e+06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.251876e+06</td>\n",
       "      <td>1327.174212</td>\n",
       "      <td>2.409528e+06</td>\n",
       "      <td>2.452100e+06</td>\n",
       "      <td>9.233807e+06</td>\n",
       "      <td>96.008792</td>\n",
       "      <td>1.513956e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>1.041550e+07</td>\n",
       "      <td>1.205467e+06</td>\n",
       "      <td>2.565764e+06</td>\n",
       "      <td>6.927194e+06</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>1.508591e+06</td>\n",
       "      <td>3.108213e+06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.654767e+06</td>\n",
       "      <td>1.680673e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.450980e+07</td>\n",
       "      <td>1.946405e+07</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.960784e+07</td>\n",
       "      <td>2973.866491</td>\n",
       "      <td>7.080406e+06</td>\n",
       "      <td>1.941748e+07</td>\n",
       "      <td>1.980198e+07</td>\n",
       "      <td>914.083472</td>\n",
       "      <td>1.941748e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>1.941748e+07</td>\n",
       "      <td>1.941748e+07</td>\n",
       "      <td>1.303053e+07</td>\n",
       "      <td>1.980198e+07</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>1.818182e+07</td>\n",
       "      <td>1.444815e+07</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>1.953654e+07</td>\n",
       "      <td>1.960784e+07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 v1            v2             v3            v4             v5  \\\n",
       "count  1.143930e+05  1.143930e+05  114393.000000  1.143930e+05  114393.000000   \n",
       "mean   9.000464e+05  4.154333e+06      -0.027764  2.290731e+06     322.415338   \n",
       "std    1.118774e+06  4.256947e+06       0.182975  2.184290e+06    1159.870074   \n",
       "min   -9.990000e+02 -9.990000e+02      -1.000000 -9.990000e+02    -999.000000   \n",
       "25%   -9.990000e+02 -9.990000e+02       0.000000 -9.990000e+02    -999.000000   \n",
       "50%    5.224367e+05  4.009687e+06       0.000000  2.692821e+06     992.289114   \n",
       "75%    1.558519e+06  7.446121e+06       0.000000  4.251876e+06    1327.174212   \n",
       "max    1.450980e+07  1.946405e+07       2.000000  1.960784e+07    2973.866491   \n",
       "\n",
       "                 v6            v7            v9            v10           v11  \\\n",
       "count  1.143930e+05  1.143930e+05  1.143930e+05  114393.000000  1.143930e+05   \n",
       "mean   1.333313e+06  1.358637e+06  5.042254e+06      85.256738  8.457066e+06   \n",
       "std    1.248859e+06  1.271173e+06  4.659935e+06      70.154956  7.459422e+06   \n",
       "min   -9.990000e+02 -9.990000e+02 -9.990000e+02    -999.000000 -9.990000e+02   \n",
       "25%   -9.990000e+02 -9.990000e+02 -9.990000e+02      48.004382 -9.990000e+02   \n",
       "50%    1.723894e+06  1.762169e+06  6.734066e+06      60.005470  1.413161e+07   \n",
       "75%    2.409528e+06  2.452100e+06  9.233807e+06      96.008792  1.513956e+07   \n",
       "max    7.080406e+06  1.941748e+07  1.980198e+07     914.083472  1.941748e+07   \n",
       "\n",
       "           ...               v115          v120          v121          v122  \\\n",
       "count      ...       1.143930e+05  1.143930e+05  1.143930e+05  1.143930e+05   \n",
       "mean       ...       5.768635e+06  7.092238e+05  1.516701e+06  3.809128e+06   \n",
       "std        ...       5.263614e+06  8.248634e+05  1.669347e+06  3.605492e+06   \n",
       "min        ...      -9.990000e+02 -9.990000e+02 -9.990000e+02 -9.990000e+02   \n",
       "25%        ...      -9.990000e+02 -9.990000e+02 -9.990000e+02 -9.990000e+02   \n",
       "50%        ...       8.299406e+06  5.425478e+05  1.357467e+06  4.727809e+06   \n",
       "75%        ...       1.041550e+07  1.205467e+06  2.565764e+06  6.927194e+06   \n",
       "max        ...       1.941748e+07  1.941748e+07  1.303053e+07  1.980198e+07   \n",
       "\n",
       "                v125          v126          v127           v129          v130  \\\n",
       "count  114393.000000  1.143930e+05  1.143930e+05  114393.000000  1.143930e+05   \n",
       "mean       33.223598  8.570540e+05  1.789827e+06       0.307589  1.067879e+06   \n",
       "std        21.666883  8.291153e+05  1.975980e+06       0.686654  1.326276e+06   \n",
       "min        -1.000000 -9.990000e+02 -9.990000e+02       0.000000 -9.990000e+02   \n",
       "25%        16.000000 -9.990000e+02 -9.990000e+02       0.000000 -9.990000e+02   \n",
       "50%        29.000000  1.146174e+06  1.493776e+06       0.000000  8.310603e+05   \n",
       "75%        47.000000  1.508591e+06  3.108213e+06       0.000000  1.654767e+06   \n",
       "max        89.000000  1.818182e+07  1.444815e+07      11.000000  1.953654e+07   \n",
       "\n",
       "               v131  \n",
       "count  1.143930e+05  \n",
       "mean   9.585897e+05  \n",
       "std    1.188139e+06  \n",
       "min   -9.990000e+02  \n",
       "25%   -9.990000e+02  \n",
       "50%    5.754002e+05  \n",
       "75%    1.680673e+06  \n",
       "max    1.960784e+07  \n",
       "\n",
       "[8 rows x 100 columns]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
